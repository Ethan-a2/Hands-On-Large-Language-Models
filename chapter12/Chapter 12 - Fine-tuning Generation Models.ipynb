{"cells":[{"cell_type":"markdown","metadata":{"id":"WpBVeU0XX8Uk"},"source":["\u003ch1\u003eChapter 12 - Fine-tuning Generation Models\u003c/h1\u003e\n","\u003ci\u003eExploring a two-step approach for fine-tuning generative LLMs.\u003c/i\u003e\n","\n","\u003ca href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"\u003e\u003cimg src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"\u003e\u003c/a\u003e\n","\u003ca href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"\u003e\u003cimg src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"\u003e\u003c/a\u003e\n","\u003ca href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"\u003e\u003cimg src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"\u003e\u003c/a\u003e\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb)\n","\n","---\n","\n","This notebook is for Chapter 12 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n","\n","---\n","\n","\u003ca href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"\u003e\n","\u003cimg src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25685,"status":"ok","timestamp":1742097672494,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"hYcAONcmHCUH","outputId":"8a5dba68-9899-4e0f-c9bb-d7e43f15b98c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/llm/Hands-On-Large-Language-Models\n","\u001b[0m\u001b[01;34mbonus\u001b[0m/      \u001b[01;34mchapter04\u001b[0m/  \u001b[01;34mchapter08\u001b[0m/  \u001b[01;34mchapter12\u001b[0m/       README.md\n","\u001b[01;34mchapter01\u001b[0m/  \u001b[01;34mchapter05\u001b[0m/  \u001b[01;34mchapter09\u001b[0m/  environment.yml  requirements_min.txt\n","\u001b[01;34mchapter02\u001b[0m/  \u001b[01;34mchapter06\u001b[0m/  \u001b[01;34mchapter10\u001b[0m/  \u001b[01;34mimages\u001b[0m/          requirements.txt\n","\u001b[01;34mchapter03\u001b[0m/  \u001b[01;34mchapter07\u001b[0m/  \u001b[01;34mchapter11\u001b[0m/  LICENSE\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive') ## mount drive\n","\n","import os\n","path = \"/content/drive/MyDrive/llm/Hands-On-Large-Language-Models\"\n","os.chdir(path)\n","print(os.getcwd())\n","\n","%ls\n"]},{"cell_type":"markdown","metadata":{"id":"vXB-Lx3JGzS0"},"source":["### [OPTIONAL] - Installing Packages on \u003cimg src=\"https://colab.google/static/images/icons/colab.png\" width=100\u003e\n","\n","If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n","\n","---\n","\n","游눠 **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n","**Runtime \u003e Change runtime type \u003e Hardware accelerator \u003e GPU \u003e GPU type \u003e T4**.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hBKVWRSDGzS1"},"outputs":[],"source":["%%capture\n","!pip install -q accelerate==0.31.0 peft==0.11.1 bitsandbytes==0.43.1 transformers==4.41.2 trl==0.9.4 sentencepiece==0.2.0"]},{"cell_type":"markdown","metadata":{"id":"v5luSSUAu_6d"},"source":["# Supervised Fine-Tuning (SFT)"]},{"cell_type":"markdown","metadata":{"id":"VPtcbw38_hVi"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":15670,"status":"error","timestamp":1742097714630,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"SqeZchJiOXdd","outputId":"bd4dac64-3794-44e6-c0e3-0d58914d8024"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-346c90bc0ddb\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load a tokenizer to use its chat template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["from transformers import AutoTokenizer\n","from datasets import load_dataset\n","\n","\n","# Load a tokenizer to use its chat template\n","template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n","\n","def format_prompt(example):\n","    \"\"\"Format the prompt to using the \u003c|user|\u003e template TinyLLama is using\"\"\"\n","\n","    # Format answers\n","    chat = example[\"messages\"]\n","    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n","\n","    return {\"text\": prompt}\n","\n","# Load and format the data using the template TinyLLama is using\n","dataset = (\n","    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n","      .shuffle(seed=42)\n","      .select(range(3_000))\n",")\n","dataset = dataset.map(format_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1742097714716,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"dtl2xZptgyDf"},"outputs":[],"source":["# Example of formatted prompt\n","print(dataset[\"text\"][2576])"]},{"cell_type":"markdown","metadata":{"id":"CyuLZGizDqUB"},"source":["## Models - Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1742097714718,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"M95Y207T7wSp"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n","\n","# 4-bit quantization configuration - Q in QLoRA\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,  # Use 4-bit precision model loading\n","    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n","    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n","    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",")\n","\n","# Load the model to train on the GPU\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","\n","    # Leave this out for regular SFT\n","    quantization_config=bnb_config,\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = \"\u003cPAD\u003e\"\n","tokenizer.padding_side = \"left\""]},{"cell_type":"markdown","metadata":{"id":"t1iGIch-sAMC"},"source":["## Configuration"]},{"cell_type":"markdown","metadata":{"id":"86o1T5n4DziD"},"source":["### LoRA Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1742097714720,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"0tYs1ZhYDyw9"},"outputs":[],"source":["from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","# Prepare LoRA Configuration\n","peft_config = LoraConfig(\n","    lora_alpha=32,  # LoRA Scaling\n","    lora_dropout=0.1,  # Dropout for LoRA Layers\n","    r=64,  # Rank\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=  # Layers to target\n","     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",")\n","\n","# prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{"id":"Zhbh7kKuD24o"},"source":["### Training Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1742097714723,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"TwxZkx80G6bO"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","output_dir = \"./results\"\n","\n","# Training arguments\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    optim=\"paged_adamw_32bit\",\n","    learning_rate=2e-4,\n","    lr_scheduler_type=\"cosine\",\n","    num_train_epochs=1,\n","    logging_steps=10,\n","    fp16=True,\n","    gradient_checkpointing=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"RtwIo5a0D6f1"},"source":["## Training!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15999,"status":"aborted","timestamp":1742097714725,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"B2D7RVihsE7Z"},"outputs":[],"source":["from trl import SFTTrainer\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    max_seq_length=512,\n","\n","    # Leave this out for regular SFT\n","    peft_config=peft_config,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Save QLoRA weights\n","trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"]},{"cell_type":"markdown","metadata":{"id":"tsIBfv1PsId-"},"source":["### Merge Adapter"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16001,"status":"aborted","timestamp":1742097714728,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"M6cPdde4Z-ks"},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    \"TinyLlama-1.1B-qlora\",\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n",")\n","\n","# Merge LoRA and base model\n","merged_model = model.merge_and_unload()"]},{"cell_type":"markdown","metadata":{"id":"jPRYGimIsM2-"},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16000,"status":"aborted","timestamp":1742097714730,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"15dJC3ZrdVnK"},"outputs":[],"source":["from transformers import pipeline\n","\n","# Use our predefined prompt template\n","prompt = \"\"\"\u003c|user|\u003e\n","Tell me something about Large Language Models.\u003c/s\u003e\n","\u003c|assistant|\u003e\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n","print(pipe(prompt)[0][\"generated_text\"])"]},{"cell_type":"markdown","metadata":{"id":"9JNfYZe9vCb8"},"source":["# Preference Tuning (PPO/DPO)"]},{"cell_type":"markdown","metadata":{"id":"ar2h9kZ9qmEG"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16000,"status":"aborted","timestamp":1742097714734,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"UlbPVO_aac33"},"outputs":[],"source":["from datasets import load_dataset\n","\n","def format_prompt(example):\n","    \"\"\"Format the prompt to using the \u003c|user|\u003e template TinyLLama is using\"\"\"\n","\n","    # Format answers\n","    system = \"\u003c|system|\u003e\\n\" + example['system'] + \"\u003c/s\u003e\\n\"\n","    prompt = \"\u003c|user|\u003e\\n\" + example['input'] + \"\u003c/s\u003e\\n\u003c|assistant|\u003e\\n\"\n","    chosen = example['chosen'] + \"\u003c/s\u003e\\n\"\n","    rejected = example['rejected'] + \"\u003c/s\u003e\\n\"\n","\n","    return {\n","        \"prompt\": system + prompt,\n","        \"chosen\": chosen,\n","        \"rejected\": rejected,\n","    }\n","\n","# Apply formatting to the dataset and select relatively short answers\n","dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n","dpo_dataset = dpo_dataset.filter(\n","    lambda r:\n","        r[\"status\"] != \"tie\" and\n","        r[\"chosen_score\"] \u003e= 8 and\n","        not r[\"in_gsm8k_train\"]\n",")\n","dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n","dpo_dataset"]},{"cell_type":"markdown","metadata":{"id":"AkCJ4CO5sQG6"},"source":["## Models - Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16000,"status":"aborted","timestamp":1742097714736,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"7YMmilm7c1-P"},"outputs":[],"source":["from peft import AutoPeftModelForCausalLM\n","from transformers import BitsAndBytesConfig, AutoTokenizer\n","\n","# 4-bit quantization configuration - Q in QLoRA\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,  # Use 4-bit precision model loading\n","    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n","    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n","    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",")\n","\n","# Merge LoRA and base model\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    \"TinyLlama-1.1B-qlora\",\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n","    quantization_config=bnb_config,\n",")\n","merged_model = model.merge_and_unload()\n","\n","# Load LLaMA tokenizer\n","model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = \"\u003cPAD\u003e\"\n","tokenizer.padding_side = \"left\""]},{"cell_type":"markdown","metadata":{"id":"iidCbaXMs1O4"},"source":["## Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16001,"status":"aborted","timestamp":1742097714738,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"m6IfkvLkylVD"},"outputs":[],"source":["from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","# Prepare LoRA Configuration\n","peft_config = LoraConfig(\n","    lora_alpha=32,  # LoRA Scaling\n","    lora_dropout=0.1,  # Dropout for LoRA Layers\n","    r=64,  # Rank\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=  # Layers to target\n","     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",")\n","\n","# prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16006,"status":"aborted","timestamp":1742097714743,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"lk-cEEd8nk27"},"outputs":[],"source":["from trl import DPOConfig\n","\n","output_dir = \"./results\"\n","\n","# Training arguments\n","training_arguments = DPOConfig(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    optim=\"paged_adamw_32bit\",\n","    learning_rate=1e-5,\n","    lr_scheduler_type=\"cosine\",\n","    max_steps=200,\n","    logging_steps=10,\n","    fp16=True,\n","    gradient_checkpointing=True,\n","    warmup_ratio=0.1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16007,"status":"aborted","timestamp":1742097714747,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"Pp3tUXhWm0pE"},"outputs":[],"source":["from trl import DPOTrainer\n","\n","# Create DPO trainer\n","dpo_trainer = DPOTrainer(\n","    model,\n","    args=training_arguments,\n","    train_dataset=dpo_dataset,\n","    tokenizer=tokenizer,\n","    peft_config=peft_config,\n","    beta=0.1,\n","    max_prompt_length=512,\n","    max_length=512,\n",")\n","\n","# Fine-tune model with DPO\n","dpo_trainer.train()\n","\n","# Save adapter\n","dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16062,"status":"aborted","timestamp":1742097714802,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"QFE4OKFvyLMe"},"outputs":[],"source":["from peft import PeftModel\n","\n","# Merge LoRA and base model\n","model = AutoPeftModelForCausalLM.from_pretrained(\n","    \"TinyLlama-1.1B-qlora\",\n","    low_cpu_mem_usage=True,\n","    device_map=\"auto\",\n",")\n","sft_model = model.merge_and_unload()\n","\n","# Merge DPO LoRA and SFT model\n","dpo_model = PeftModel.from_pretrained(\n","    sft_model,\n","    \"TinyLlama-1.1B-dpo-qlora\",\n","    device_map=\"auto\",\n",")\n","dpo_model = dpo_model.merge_and_unload()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16071,"status":"aborted","timestamp":1742097714814,"user":{"displayName":"達던씷","userId":"18135480703371265315"},"user_tz":-480},"id":"zAkwJcHYmxr4"},"outputs":[],"source":["from transformers import pipeline\n","\n","# Use our predefined prompt template\n","prompt = \"\"\"\u003c|user|\u003e\n","Tell me something about Large Language Models.\u003c/s\u003e\n","\u003c|assistant|\u003e\n","\"\"\"\n","\n","# Run our instruction-tuned model\n","pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n","print(pipe(prompt)[0][\"generated_text\"])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}