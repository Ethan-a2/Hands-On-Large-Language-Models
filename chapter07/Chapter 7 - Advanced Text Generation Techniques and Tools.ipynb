{"cells":[{"cell_type":"markdown","metadata":{"id":"-ETtu9CvVMDR"},"source":["<h1>Chapter 7 - Advanced Text Generation Techniques and Tools</h1>\n","<i>Going beyond prompt engineering.</i>\n","\n","<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n","<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n","<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb)\n","\n","---\n","\n","This notebook is for Chapter 7 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n","\n","---\n","\n","<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n","<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') ## mount drive\n","\n","import os\n","path = \"/content/drive/MyDrive/llm/Hands-On-Large-Language-Models\"\n","os.chdir(path)\n","print(os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igajR2YWSdLb","executionInfo":{"status":"ok","timestamp":1742252062545,"user_tz":-480,"elapsed":1903,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}},"outputId":"e85648fd-a187-4d8b-f816-79f65f3259c9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/llm/Hands-On-Large-Language-Models\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"U88VN3FASW6x","executionInfo":{"status":"ok","timestamp":1742252062558,"user_tz":-480,"elapsed":11,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtv-0ksVSUJ4"},"source":["### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n","\n","If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n","\n","---\n","\n","ðŸ’¡ **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n","**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n","\n","---\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Z4epm8ezSUJ5","executionInfo":{"status":"ok","timestamp":1742252227557,"user_tz":-480,"elapsed":165005,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["#%%capture\n","!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n"]},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sc8Y8BBzVyQO","executionInfo":{"status":"ok","timestamp":1742252898397,"user_tz":-480,"elapsed":410266,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}},"outputId":"e371913a-a594-47fb-afd3-4b93e15e74d8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python==0.2.69\n","  Using cached llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n","  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.2)\n","Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp311-cp311-linux_x86_64.whl size=55713995 sha256=8d9a4cf4f936e61d3429095d0cfc6377f4d822dc10df49890f42e71758a792b5\n","  Stored in directory: /root/.cache/pip/wheels/e8/1b/ff/b4dba97fbd16e731705b262602ba8f3b672bf4bde54ea0c104\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"]}]},{"cell_type":"markdown","metadata":{"id":"rerbJgwAigbK"},"source":["# Loading an LLM"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oOqU0eASUJ7","executionInfo":{"status":"ok","timestamp":1742252357179,"user_tz":-480,"elapsed":129623,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}},"outputId":"2674c515-eb0f-451f-ba22-534961347f30"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-17 22:57:07--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n","Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.23, 18.164.174.55, ...\n","Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1742255827&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjI1NTgyN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=loyyQywD20j1TbjaeuEHT%7EQOrVUnGFC-RIMCxZpAonrc2Leh0uSPxXqTrOTwrs9jWvkdj5By7y4Y8ntSTGDAWrwQNTCHLkwob3biwVnz%7E57pqGXyO62RMPtTloufP0rBBA9KP3qSniuYv350xrPN7qa%7EJ7L7VSdH4JOy8T5ATJWrsjGADXT7C7VYE6Vye5lDvnz2VPT3sDFkXlqCn8MIvZFkz88VYR1hjhpyJCc51LneUPAUn5VOQwEMxKVu9KDzaqb3pSrqnZ02plTbUIV1Q1TVVLU642vRHe-rPdmvGrxIm0avLefsfrIBzpcxiwzH06x3GsI4BSKv1VOUkT2%7ETA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n","--2025-03-17 22:57:07--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1742255827&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjI1NTgyN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=loyyQywD20j1TbjaeuEHT%7EQOrVUnGFC-RIMCxZpAonrc2Leh0uSPxXqTrOTwrs9jWvkdj5By7y4Y8ntSTGDAWrwQNTCHLkwob3biwVnz%7E57pqGXyO62RMPtTloufP0rBBA9KP3qSniuYv350xrPN7qa%7EJ7L7VSdH4JOy8T5ATJWrsjGADXT7C7VYE6Vye5lDvnz2VPT3sDFkXlqCn8MIvZFkz88VYR1hjhpyJCc51LneUPAUn5VOQwEMxKVu9KDzaqb3pSrqnZ02plTbUIV1Q1TVVLU642vRHe-rPdmvGrxIm0avLefsfrIBzpcxiwzH06x3GsI4BSKv1VOUkT2%7ETA__&Key-Pair-Id=K24J24Z295AEI9\n","Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.65.3.87, 18.65.3.76, 18.65.3.59, ...\n","Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.65.3.87|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7643295904 (7.1G) [binary/octet-stream]\n","Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n","\n","Phi-3-mini-4k-instr 100%[===================>]   7.12G  52.9MB/s    in 2m 9s   \n","\n","2025-03-17 22:59:16 (56.6 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n","\n"]}],"source":["!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n","\n","# If this command does not work for you, you can use the link directly to download the model\n","# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LQcht_ZFijW7","colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"status":"error","timestamp":1742252358443,"user_tz":-480,"elapsed":1262,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}},"outputId":"0fefb308-3994-4c39-930b-ad1752a707bc"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaGrammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-746052f0fb2f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make sure the model path is correct for your system!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m llm = LlamaCpp(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Phi-3-mini-4k-instruct-fp16.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_gpu_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_decorators_v1.py\u001b[0m in \u001b[0;36m_wrapper1\u001b[0;34m(values, _)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# mode='before' for pydantic-core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wrapper1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRootValidatorValues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcore_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidationInfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRootValidatorValues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapper1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/utils/pydantic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;31m# Call the decorated function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/llms/llamacpp.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaGrammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0;34m\"Could not import llama-cpp-python library. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;34m\"Please install the llama-cpp-python library to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from langchain import LlamaCpp\n","\n","# Make sure the model path is correct for your system!\n","llm = LlamaCpp(\n","    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n","    n_gpu_layers=-1,\n","    max_tokens=500,\n","    n_ctx=2048,\n","    seed=42,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":297996,"status":"aborted","timestamp":1742252358434,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"3SNhQF9WthzV"},"outputs":[],"source":["llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"]},{"cell_type":"markdown","metadata":{"id":"Wwx2AIuGfCoP"},"source":["### Chains"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kF--Q5me_-X1","executionInfo":{"status":"aborted","timestamp":1742252358456,"user_tz":-480,"elapsed":298016,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain import PromptTemplate\n","\n","# Create a prompt template with the \"input_prompt\" variable\n","template = \"\"\"<s><|user|>\n","{input_prompt}<|end|>\n","<|assistant|>\"\"\"\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"input_prompt\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogWsGeg6hElt","executionInfo":{"status":"aborted","timestamp":1742252358461,"user_tz":-480,"elapsed":298019,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["basic_chain = prompt | llm"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298021,"status":"aborted","timestamp":1742252358465,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"KINQxKAINXgG"},"outputs":[],"source":["# Use the chain\n","basic_chain.invoke(\n","    {\n","        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n","    }\n",")"]},{"cell_type":"markdown","metadata":{"id":"sSMBMRxB8gFW"},"source":["### Multiple Chains"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298026,"status":"aborted","timestamp":1742252358472,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"wrUKuHt_OLpe"},"outputs":[],"source":["from langchain import LLMChain\n","\n","# Create a chain for the title of our story\n","template = \"\"\"<s><|user|>\n","Create a title for a story about {summary}. Only return the title.<|end|>\n","<|assistant|>\"\"\"\n","title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n","title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298029,"status":"aborted","timestamp":1742252358477,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"igFIyg73OtaL"},"outputs":[],"source":["title.invoke({\"summary\": \"a girl that lost her mother\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTtFEmANOhyE","executionInfo":{"status":"aborted","timestamp":1742252358480,"user_tz":-480,"elapsed":298030,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Create a chain for the character description using the summary and title\n","template = \"\"\"<s><|user|>\n","Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n","<|assistant|>\"\"\"\n","character_prompt = PromptTemplate(\n","    template=template, input_variables=[\"summary\", \"title\"]\n",")\n","character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xjf-avW8NAqZ","executionInfo":{"status":"aborted","timestamp":1742252358510,"user_tz":-480,"elapsed":298059,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Create a chain for the story using the summary, title, and character description\n","template = \"\"\"<s><|user|>\n","Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n","<|assistant|>\"\"\"\n","story_prompt = PromptTemplate(\n","    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",")\n","story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epNudKyyPClO","executionInfo":{"status":"aborted","timestamp":1742252358512,"user_tz":-480,"elapsed":298059,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Combine all three components to create the full chain\n","llm_chain = title | character | story"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298060,"status":"aborted","timestamp":1742252358514,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"b44ZR0vXRaAo"},"outputs":[],"source":["llm_chain.invoke(\"a girl that lost her mother\")"]},{"cell_type":"markdown","metadata":{"id":"7UQ-DZ71P-D-"},"source":["# Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298060,"status":"aborted","timestamp":1742252358516,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"-15Eoey5EJUO"},"outputs":[],"source":["# Let's give the LLM our name\n","basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298063,"status":"aborted","timestamp":1742252358521,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"N42wQRl-Lykt"},"outputs":[],"source":["# Next, we ask the LLM to reproduce the name\n","basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"]},{"cell_type":"markdown","metadata":{"id":"PfqATEZjMgET"},"source":["## ConversationBuffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zoo0PA1fUs70","executionInfo":{"status":"aborted","timestamp":1742252358525,"user_tz":-480,"elapsed":298066,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Create an updated prompt template to include a chat history\n","template = \"\"\"<s><|user|>Current conversation:{chat_history}\n","\n","{input_prompt}<|end|>\n","<|assistant|>\"\"\"\n","\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=[\"input_prompt\", \"chat_history\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgGMS1S9saLi","executionInfo":{"status":"aborted","timestamp":1742252358527,"user_tz":-480,"elapsed":298066,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain.memory import ConversationBufferMemory\n","\n","# Define the type of Memory we will use\n","memory = ConversationBufferMemory(memory_key=\"chat_history\")\n","\n","# Chain the LLM, Prompt, and Memory together\n","llm_chain = LLMChain(\n","    prompt=prompt,\n","    llm=llm,\n","    memory=memory\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298067,"status":"aborted","timestamp":1742252358529,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"mltR_GtkiqDZ"},"outputs":[],"source":["# Generate a conversation and ask a basic question\n","llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298067,"status":"aborted","timestamp":1742252358531,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"h-je1rmy3dx4"},"outputs":[],"source":["# Does the LLM remember the name we gave it?\n","llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"]},{"cell_type":"markdown","metadata":{"id":"Sw3ELCg6Rpsk"},"source":["## ConversationBufferMemoryWindow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0DRT7kjRtiC","executionInfo":{"status":"aborted","timestamp":1742252358534,"user_tz":-480,"elapsed":298068,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain.memory import ConversationBufferWindowMemory\n","\n","# Retain only the last 2 conversations in memory\n","memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n","\n","# Chain the LLM, Prompt, and Memory together\n","llm_chain = LLMChain(\n","    prompt=prompt,\n","    llm=llm,\n","    memory=memory\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298070,"status":"aborted","timestamp":1742252358537,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"CBY69vvcR1Qq"},"outputs":[],"source":["# Ask two questions and generate two conversations in its memory\n","llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n","llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298072,"status":"aborted","timestamp":1742252358541,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"nvSLfKWpR5h5"},"outputs":[],"source":["# Check whether it knows the name we gave it\n","llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298074,"status":"aborted","timestamp":1742252358544,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"YW7qEyctcqeJ"},"outputs":[],"source":["# Check whether it knows the age we gave it\n","llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"]},{"cell_type":"markdown","metadata":{"id":"tSb5OnANMhu2"},"source":["## ConversationSummary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWHZlJUbwpqE","executionInfo":{"status":"aborted","timestamp":1742252358545,"user_tz":-480,"elapsed":298073,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Create a summary prompt template\n","summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n","\n","Current summary:\n","{summary}\n","\n","new lines of conversation:\n","{new_lines}\n","\n","New summary:<|end|>\n","<|assistant|>\"\"\"\n","summary_prompt = PromptTemplate(\n","    input_variables=[\"new_lines\", \"summary\"],\n","    template=summary_prompt_template\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qg1HAgxZMkbO","executionInfo":{"status":"aborted","timestamp":1742252358547,"user_tz":-480,"elapsed":298074,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain.memory import ConversationSummaryMemory\n","\n","# Define the type of memory we will use\n","memory = ConversationSummaryMemory(\n","    llm=llm,\n","    memory_key=\"chat_history\",\n","    prompt=summary_prompt\n",")\n","\n","# Chain the LLM, prompt, and memory together\n","llm_chain = LLMChain(\n","    prompt=prompt,\n","    llm=llm,\n","    memory=memory\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298073,"status":"aborted","timestamp":1742252358548,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"2klIk9CpVSH0"},"outputs":[],"source":["# Generate a conversation and ask for the name\n","llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n","llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298075,"status":"aborted","timestamp":1742252358552,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"_VdOH_I-V-Fy"},"outputs":[],"source":["# Check whether it has summarized everything thus far\n","llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298076,"status":"aborted","timestamp":1742252358554,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"n1_LlvrVX9HL"},"outputs":[],"source":["# Check what the summary is thus far\n","memory.load_memory_variables({})"]},{"cell_type":"markdown","metadata":{"id":"BG5sJa1qvS4N"},"source":["# Agents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcBt8bZM56dM","executionInfo":{"status":"aborted","timestamp":1742252358556,"user_tz":-480,"elapsed":298076,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["import os\n","from langchain_openai import ChatOpenAI\n","\n","# Load OpenAI's LLMs with LangChain\n","os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n","openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmRZu8DO2p6k","executionInfo":{"status":"aborted","timestamp":1742252358558,"user_tz":-480,"elapsed":298076,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["# Create the ReAct template\n","react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n","\n","{tools}\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [{tool_names}]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: {input}\n","Thought:{agent_scratchpad}\"\"\"\n","\n","prompt = PromptTemplate(\n","    template=react_template,\n","    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NV-ssNa-4zOK","executionInfo":{"status":"aborted","timestamp":1742252358559,"user_tz":-480,"elapsed":298075,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain.agents import load_tools, Tool\n","from langchain.tools import DuckDuckGoSearchResults\n","\n","# You can create the tool to pass to an agent\n","search = DuckDuckGoSearchResults()\n","search_tool = Tool(\n","    name=\"duckduck\",\n","    description=\"A web search engine. Use this to as a search engine for general queries.\",\n","    func=search.run,\n",")\n","\n","# Prepare tools\n","tools = load_tools([\"llm-math\"], llm=openai_llm)\n","tools.append(search_tool)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tAr1962vS4T","executionInfo":{"status":"aborted","timestamp":1742252358561,"user_tz":-480,"elapsed":298075,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"}}},"outputs":[],"source":["from langchain.agents import AgentExecutor, create_react_agent\n","\n","# Construct the ReAct agent\n","agent = create_react_agent(openai_llm, tools, prompt)\n","agent_executor = AgentExecutor(\n","    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":298075,"status":"aborted","timestamp":1742252358563,"user":{"displayName":"ä¹”å³°","userId":"18135480703371265315"},"user_tz":-480},"id":"QSU6ECdYBOOm"},"outputs":[],"source":["# What is the Price of a MacBook Pro?\n","agent_executor.invoke(\n","    {\n","        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n","    }\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}